{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac92f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           hello|\n",
      "+----------------+\n",
      "|welcome to spark|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df=spark.sql(\"Select 'welcome to spark' as hello\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bfcdfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (349665686.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hemalatha Karumuri\\AppData\\Local\\Temp\\ipykernel_35680\\349665686.py\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    val num = Array(1,2,3,4,5)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext as sc\n",
    "import pyspark\n",
    "val num = Array(1,2,3,4,5)\n",
    "val n=sc.parallelize(num)\n",
    "n.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac06597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.16\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import random\n",
    "sc = SparkContext.getOrCreate();\n",
    "num_samples = 10000\n",
    "def inside(p):     \n",
    "    x,y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pione = 4 * count / num_samples\n",
    " \n",
    "print(pione)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f78e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|year of experiance|salary|\n",
      "+------------------+------+\n",
      "|               1.1| 40000|\n",
      "|               1.2| 44000|\n",
      "|               2.0| 50000|\n",
      "|               3.0| 60000|\n",
      "|               4.0| 75000|\n",
      "|               5.0| 80000|\n",
      "+------------------+------+\n",
      "\n",
      "root\n",
      " |-- year of experiance: double (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession\\\n",
    "       .builder\\\n",
    "       .appName(\"Python spark sql basic examples\")\\\n",
    "        .config(\"spark.come.config.option\",\"some-values\")\\\n",
    "        .getOrCreate()\n",
    "df=spark.read.format('com.databricks.spark.csv').\\\n",
    "              options(header='true',\\\n",
    "                     inferschema='true').\\\n",
    "            load(\"H:/Hexaware/python-data/salarydata.csv\",header=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294780c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create RDD uisng parallelize() method\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
    "(4, 5, 6, 'd e f'),\n",
    "(7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526cde16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "# Create RDD           \n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "# Create DataFrame from RDD\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2571f897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e8f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n",
      "| Id| Name|Salary|DepartmentId|\n",
      "+---+-----+------+------------+\n",
      "|  1|  Joe| 70000|           1|\n",
      "|  2|Henry| 80000|           2|\n",
      "|  3|  Sam| 60000|           2|\n",
      "|  4|  Max| 90000|           1|\n",
      "+---+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By using createDataFrame( ) function\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "Employee = spark.createDataFrame([\n",
    "('1', 'Joe', '70000', '1'),\n",
    "('2', 'Henry', '80000', '2'),\n",
    "('3', 'Sam', '60000', '2'),\n",
    "('4', 'Max', '90000', '1')],\n",
    "['Id', 'Name', 'Salary','DepartmentId'])\n",
    "Employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a42a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Name| Dept|\n",
      "+------+-----+\n",
      "|  Hema|  CSE|\n",
      "| Paani|  CSE|\n",
      "|  Ravi|  EEE|\n",
      "|Chandu|  ECE|\n",
      "|  Hari|Civil|\n",
      "+------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|      Name| Dept|\n",
      "+----------+-----+\n",
      "|  Hema CSE|  CSE|\n",
      "| Paani CSE|  CSE|\n",
      "|  Ravi EEE|  EEE|\n",
      "|Chandu ECE|  ECE|\n",
      "|Hari Civil|Civil|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+---+\n",
      "|      Name| Dept|age|\n",
      "+----------+-----+---+\n",
      "|  Hema CSE|  CSE| 35|\n",
      "| Paani CSE|  CSE| 38|\n",
      "|  Ravi EEE|  EEE| 33|\n",
      "|Chandu ECE|  ECE| 49|\n",
      "|Hari Civil|Civil| 31|\n",
      "+----------+-----+---+\n",
      "\n",
      "+----------+-----+---+\n",
      "|      Name| Dept|age|\n",
      "+----------+-----+---+\n",
      "|  Hema CSE|  CSE| 35|\n",
      "| Paani CSE|  CSE| 38|\n",
      "|  Ravi EEE|  EEE| 33|\n",
      "|Chandu ECE|  ECE| 49|\n",
      "|Hari Civil|Civil| 31|\n",
      "+----------+-----+---+\n",
      "\n",
      "+----------+-----+---+\n",
      "|      Name| Dept|age|\n",
      "+----------+-----+---+\n",
      "|Hari Civil|Civil| 31|\n",
      "|  Ravi EEE|  EEE| 33|\n",
      "|  Hema CSE|  CSE| 35|\n",
      "| Paani CSE|  CSE| 38|\n",
      "|Chandu ECE|  ECE| 49|\n",
      "+----------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, lit, floor, rand\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"ComplexETL\").getOrCreate()\n",
    "# Define the external source and target paths\n",
    "source_path = \"H:/Hexaware/python-data/studentdata.csv\" # Update with your actual source file path\n",
    "target_path = \"H:/Hexaware/python-data/data1.csv\" # Update with your desired target file path\n",
    "# Extract: Read data from an external CSV file\n",
    "df = spark.read.csv(source_path, header=True,schema = 'Name String,Dept String')\n",
    "df.show()\n",
    "# Transformation 1: Concatenate First and Last Names\n",
    "df = df.withColumn(\"Name\", concat(col(\"Name\"), lit(\" \"), col(\"Dept\")))\n",
    "df.show()\n",
    "#adding age column\n",
    "df = df.withColumn(\"age\", floor(lit(20) + rand() * lit(31)))\n",
    "df.show()\n",
    "# Transformation 3: Filter by Age (age >= 30)\n",
    "df = df.filter(col(\"age\") >= 30)\n",
    "df.show()\n",
    "# Transformation 5: Sort by Age\n",
    "df = df.orderBy(\"age\")\n",
    "df.show()\n",
    "# Save the transformed data to an external CSV file\n",
    "#df.write.csv(target_path, mode=\"overwrite\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddf3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
